{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LangChain**\n",
    "\n",
    "**LangChain** is one of the most popular open-source frameworks for building applications powered by Large Language Models (LLMs). It simplifies the development, production, and deployment of LLM-based applications by providing modular and standardized components.\n",
    "\n",
    "LangChain enables developers to **seamlessly integrate LLMs with external data sources, tools, and environments**, such as databases, APIs, knowledge bases, and more ‚Äî making it ideal for building sophisticated generative AI (GenAI) systems.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ† **Why LangChain?**\n",
    "\n",
    "LangChain came into the spotlight to solve a crucial challenge:\n",
    "\n",
    "> \"How do we use LLMs *effectively* in real-world applications?\"\n",
    "\n",
    "As LLMs from providers like **OpenAI, Meta, Google**, and others became widely available, developers needed tools to manage:\n",
    "\n",
    "* **Prompt chaining and orchestration**\n",
    "* **Model abstraction and swapping**\n",
    "* **Tool integration (e.g., search engines, calculators)**\n",
    "* **Memory and context management**\n",
    "* **Debugging, tracing, and monitoring**\n",
    "\n",
    "LangChain provides a **unified framework** to handle all of the above.\n",
    "\n",
    "---\n",
    "\n",
    "### üìú **History & Evolution**\n",
    "\n",
    "A few years ago, multiple companies released powerful LLMs. However, developers struggled to build scalable and maintainable applications using these models due to a lack of standardized tooling.\n",
    "\n",
    "LangChain was created to address this gap. It started as a **Python library** focused on chaining prompts together, but quickly evolved into a **full-fledged ecosystem** for GenAI app development, supporting:\n",
    "\n",
    "* **Multi-modal applications** (text, images, etc.)\n",
    "* **Agent-based workflows**\n",
    "* **Tool and API calling**\n",
    "* **LLM integration with business logic**\n",
    "\n",
    "LangChain now supports both **Python** and **JavaScript/TypeScript**, making it accessible to a wider range of developers.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç **LangChain Ecosystem: Key Components**\n",
    "\n",
    "1. **LLMs & Chat Models:** Interfaces to call models from OpenAI, Anthropic, Cohere, Google, and more.\n",
    "2. **Prompts:** Templates and dynamic prompt creation with support for variables and context.\n",
    "3. **Chains:** Logic for chaining calls and decisions between models and tools.\n",
    "4. **Agents:** Autonomous systems that can reason, choose tools, and take actions.\n",
    "5. **Memory:** Persistent or contextual memory across interactions.\n",
    "6. **Tools:** Integration with external APIs, search engines, Python code execution, etc.\n",
    "7. **Callbacks & Tracing:** Debugging and monitoring tools like LangSmith.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **Use Cases**\n",
    "\n",
    "* AI-powered **chatbots and virtual assistants**\n",
    "* **Document Q\\&A** systems using Retrieval-Augmented Generation (RAG)\n",
    "* **Customer support automation**\n",
    "* **Data extraction and summarization**\n",
    "* **AI agents** that use tools, APIs, or custom logic\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ **Why LangChain Matters Today**\n",
    "\n",
    "As GenAI becomes central to modern software, LangChain helps bridge the gap between LLM capabilities and real-world application needs by focusing on:\n",
    "\n",
    "* **Modularity**\n",
    "* **Transparency**\n",
    "* **Observability**\n",
    "* **Tooling support**\n",
    "\n",
    "---\n",
    "\n",
    "* **LangChain Community**\n",
    "    It is in the core component of LangChain. It helps in accessing all the models available from different sources, like - Google, Huggingface, Meta, etc. \n",
    "    Helps in retriever, vector store etc. \n",
    "\n",
    "* **RAG** - Retrieval Augmented Generation\n",
    "\n",
    "* **Basic Components**\n",
    "    * ***RAG - Retrieval Augmented Generation***\n",
    "        (Ques - Suppose I have thousands of pages of pdf. Now with your GENAI app, when you ask any question, it should be capable of answering that using the information from pdfs. )\n",
    "        Steps to solve this\n",
    "        \n",
    "        **MODULE 1** \n",
    "        1. **LOAD** the data from different Data Sources. You should know how to inject data from different data sources.(Data Source).\n",
    "        2. **SPLIT** breaking this data into smaller chunks(text). Why because, LLM models has limitation wrt the context size. (Data Transformation)\n",
    "        3. **EMBED** these text chunks to vectors.(Algorithm - Similarity search. Each company has its own embedding techniques.)\n",
    "        4. **STORE** Store vectors in vector store. (FAISS, CHROMADB, etc.)\n",
    "        \n",
    "        **MODULE 2** \n",
    "        1. User asks a Question. Question goes to Vector Store DB.\n",
    "        2. **RETRIEVAL CHAIN** retrieval chain is an interface which is responsible for querying the Vector Store DB. Answer is called as **Context info**\n",
    "        3. Context info is combined with the Prompt(order/msg i want to give to the app) = Goes to LLM  \n",
    "        4. From LLM , we finally gets our answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
